# Морфологический анализ текста и префиксное дерево слогов: исследование и техническое задание

## 1. Основные понятия

### 1.1 Морфологический анализ

**Морфологический анализ** – это процесс определения грамматических свойств слова (части речи и различных морфологических категорий) и приведения его к начальной форме (лемме). Иными словами, для каждого слова определяется, чем оно является с точки зрения грамматики (например, существительное мужского рода, именительного падежа, единственного числа) и какая у него базовая форма (для приведенного примера – именительный падеж, ед. число). Например, слово *«яблочками»* при морфологическом разборе определяется как существительное, нарицательное, неодушевлённое, среднего рода, множественного числа, творительного падежа, с начальной формой «яблочко». Морфологический анализ играет важную роль в NLP-задачах: результаты разборов используются для дальнейшей обработки текста, обучения моделей или, например, для поиска по словам во всех их формах (лемматизация позволяет поисковику находить слово независимо от его падежа или числа).

При компьютерном анализе существуют специальные программы – **морфологические анализаторы**, автоматически выполняющие такой разбор. В русском языке наиболее известны библиотека **pymorphy2** (Python) и программа **Mystem** (Яндекс), которые работают на основе большого словаря словоформ и набора правил. Современные подходы включают и нейросетевые модели, но классический морфоанализ (как в pymorphy2/Mystem) не учитывает контекст и разбирает каждое слово по отдельности по его написанию.

### 1.2 Омонимы

**Омонимы** – это слова, которые пишутся и звучат одинаково, но имеют разное значение (или грамматическую категорию). Классические примеры омонимов в русском языке: *«лук»* (оружие) и *«лук»* (овощ), *«ключ»* (родник) и *«ключ»* (инструмент), *«течь»* (сущ. «протекание») и *«течь»* (гл. «протекать»). В контексте морфологического анализа омонимия проявляется как **морфологическая неоднозначность**: одна и та же словоформа может соответствовать нескольким различным разборам. Например, словоформа *«стали»* может быть формой глагола *«стать»* (мн.ч., прош.вр. – *«они стали»*) или существительного *«сталь»* (ед.ч. род.падеж, или мн.ч. им.падеж, и т.д.). Такие случаи называются омонимичными словоформами или омоформами. Омонимия значительно затрудняет автоматический разбор: без контекста неясно, какой смысл и грамматическая роль слова в конкретном предложении.

### 1.3 Снятие омонимии

**Снятие омонимии** – это процесс разрешения неоднозначности, выбор правильного значения или грамматического разбора слова из нескольких возможных вариантов. В морфологическом анализе под снятием омонимии обычно понимают автоматический выбор корректной части речи и граммем для слова на основе контекста (также известное как **дизамбигуация** или POS-теггинг). Например, для слова *«стали»* в предложении *«Рабочие стали у станков»* контекст подсказывает, что это глагол (от *«стать»*), а в предложении *«Из стали делают инструменты»* – что это существительное (металл).

Существует множество алгоритмов для снятия морфологической омонимии. Классические подходы – статистические модели, такие как скрытые марковские модели (HMM) или условные случайные поля, обученные на размеченных корпусах, – по сути выполняют часть речи разметку текста. В последние годы активно применяются нейронные сети для этой задачи. Однако простой подход – выбрать наиболее распространённый вариант – тоже часто используется. Например, pymorphy2 возвращает список разборов с оценкой вероятности (на основе частот в корпусе), и **самый вероятный разбор можно принять как правильный**. В общем случае для полного разрешения неоднозначности требуется учитывать соседние слова, синтаксис и смысл предложения. Впрочем, даже человек в некоторых случаях без контекста не может различить омонимы (например, фраза *«Эти типы стали есть в цехе»* остаётся двусмысленной даже в контексте). Поэтому в сложных случаях снимают омонимию с помощью дополнительных правил или более глубокого анализа текста.

### 1.4 Префиксное дерево (Trie)

**Префиксное дерево (trie)** – это структура данных в виде дерева, предназначенная для хранения набора строк (например, слов) таким образом, что общие префиксы строк хранятся один раз. Каждый путь от корня до некоторого узла соответствует префиксу, а путь до помеченного терминального узла – полному ключу (строке). В классическом трие на рёбрах записаны символы: переходя по последовательности символов от корня к листу, мы читаем хранимое слово. Например, если в trie хранятся слова «tea», «ted» и «ten», они будут разветвляться от общего префикса «te…». Такая структура позволяет эффективно проверять, есть ли строка в словаре, и быстро находить все строки с общим префиксом. В узлах дерева обычно сохраняют дополнительную информацию, связанную с ключом (например, отметку конца слова и ссылку на данные). Важно, что **ключ не хранится целиком в одном узле** – он определяется расположением узла в дереве. Для навигации используется последовательность символов входного ключа, спускаясь по дереву от корня. Одно префиксное дерево может хранить тысячи и миллионы ключей, при этом поиск по префиксу имеет сложность *O(n)*, где *n* – длина искомой строки (не зависит от числа ключей). Префиксные деревья широко применяются в задачах автодополнения, проверки орфографии, алгоритме Ахо-Корасика, и др., благодаря компактному хранению и быстрому поиску по префиксам.

### 1.5 Ключи как последовательности слогов

В традиционных tries минимальной единицей на рёбрах служит символ (буква). Однако в нашем случае предлагается использовать в качестве ключей **последовательности слогов** слов. Это означает, что строка-ключ для вставки в дерево – не само слово целиком и не отдельные буквы, а **последовательность слогов, на которые разбивается слово**. Например, слово «префиксное» делится на слоги как «пре-фикс-но-е» – эти слоги станут последовательностью ключевых элементов. В префиксном дереве первый узел от корня будет соответствовать слогу «пре», следующий уровень – слогу «фикс», затем «но», затем «е». Таким образом, путь «пре» → «фикс» → «но» → «е» будет обозначать слово «префиксное». Другие слова, начинающиеся на «пре-…», будут разделять начальный узел. Использование слогов в качестве шагов дерева группирует слова по схожим звуковым структурам. Это нестандартный подход (чаще в деревья помещают либо отдельные буквы, либо целые слова), но он может быть полезен, например, для анализа фонетических шаблонов или рифм: слова с общими начальными слогами окажутся близко в дереве.

При реализации такого дерева нужно определить, что считать слогом – для этого важен алгоритм разбиения слов на слоги (см. следующий пункт). В результате **каждый уникальный слог выступает как отдельный узел**, а ветвления происходят в точках, где слова отличаются по слоговой структуре. Ключом полного пути будет последовательность нескольких слогов, соответствующая целому слову. В терминальных узлах (оканчивающих слово) можно хранить информацию о слове – например, его исходный текст или ссылку на морфологические данные.

### 1.6 Алгоритмы деления слов на слоги

**Деление слова на слоги** – нетривиальная задача, так как в разных языках существуют разные правила слогораздела. Для русского языка действует простое правило: количество слогов равно количеству гласных букв. Однако расположение границ слогов зависит от сочетания согласных и гласных. Основные принципы русской слоговой структуры:

* **Каждый слог должен содержать хотя бы одну гласную** (она образует слоговое ядро).
* **Максимальная звучность в середине слога**: слоги обычно делятся по принципу восходящей и нисходящей звучности звуков (гласные наиболее звучные, сонорные менее, шумные согласные ещё менее и т.д.). На практике это означает, что при разбиении групп согласных одна из них относится к предыдущему слогу, а остальные отходят к следующему, чтобы следующий слог начинался с максимально возможной согласной группы.

Существуют готовые алгоритмы. Один из них: **алгоритм по расстоянию между гласными**. Суть его в следующем. Имеем индекс предыдущей гласной `a` и следующей гласной `b` в слове:

* Если между двумя соседними гласными нет согласных (расстояние `b - a = 1`), то слоговая граница ставится прямо перед второй гласной (то есть две гласные не образуют один слог, например в «саоран» будет «са-о-ран»).
* Если между гласными одна буква (`b - a = 2`, то есть структура *гласная–согласная–гласная*, например «молоко» между *о*...*о* одна *л*), то разрыв делается **перед этой согласной** (схема: VCV → V-CV). Например, «мо-ло-ко».
* Если между гласными несколько букв (`b - a > 2`, например группа из двух и более согласных), то слоговую границу ставят после первой согласной из этой группы (`a + 2` позиции от первой гласной). То есть, при последовательности VCCV разбиваем как VC-CV: одна согласная остаётся в предыдущем слоге, остальные уходят в следующий. Например, «воль-тер», «ин-стру-мент».

Этот алгоритм можно доработать, учитывая буквы **Й, Ь, Ъ**, которые не обозначают самостоятельного гласного звука. Их обычно присоединяют к соседним слогам по особым правилам (например, *«йо»* обычно не разрывается). Существуют улучшенные версии алгоритма, учитывающие эти случаи. В сообществе разработчиков также известен алгоритм П. Христова (в модификации Дымченко и Варсанофьева), применяемый для автоматического переноса слов, но авторы отмечают наличие частных сомнительных правил в нём, тогда как приведённый выше алгоритм более прямолинеен и производителен.

На практике, для реализации можно использовать готовые решения. Например, пакет **rusyllab** предоставляет функцию разбивки на слоги на основе набора правил. Он реализует жадный алгоритм с ручными правилами и обычно справляется неплохо. Пример: `"Голодная кошка ловит мышку"` разбивается rusyllab на последовательность токенов `["Го", "лод", "на", "я", " ", "кош", "ка", " ", "ло", "вит", " ", "мыш", "ку"]`, что соответствует слогам `Го-лод-на-я кош-ка ло-вит мыш-ку`. Такой инструмент удобен, чтобы не реализовывать все правила самостоятельно. Важно понимать, что в русском языке **слогораздел часто совпадает с возможными местами переноса** слова по слогам, но не всегда идентичен морфемным границам: алгоритм опирается на фонетику, а не на состав слова.

## 2. Библиотеки для морфологического анализа русского языка

Для автоматического морфологического разбора русского текста доступны несколько библиотек. Рассмотрим две упомянутые: **pymorphy2** и **Natasha**, и сравним их возможности, особенно в контексте снятия омонимии.

### 2.1 Pymorphy2 (морфологический анализатор)

Pymorphy2 – это популярная Python-библиотека для морфологического анализа, основанная на словаре (OpenCorpora) и наборе правил склонения/спряжения. При инициализации анализатор загружает \~15-20 МБ данных словаря в память. Pymorphy2 умеет:

* получать все возможные разборы слова (часть речи и граммемы),
* выдавать нормальную форму (лемму),
* склонять или спрягать слово в заданную форму.

**Как работает анализ:** Pymorphy2 ищет слово в словаре известных словоформ. Если слово находитcя, возвращается одна или несколько словарных форм с тегами (теги в формате OpenCorpora). Если слово неизвестно, анализатор пытается угадать разбор по похожим окончаниям (предсказание). Например, `morph.parse("стали")` выдаст 5 разборов слова «стали» – один глагол (прош. вр. мн. ч. от «стать») и четыре существительных (разные формы слова «сталь»). У каждого разбора есть вероятностный **score** – оценка правдоподобия на основе частот корпуса. Pymorphy2 по умолчанию сортирует разборы по убыванию score, первый обычно наиболее вероятный. В примере «стали» глагол имеет score \~0.984, а каждый из существительных \~0.003, поэтому глагольный разбор идёт первым.

**Снятие омонимии в Pymorphy2:** Сам по себе pymorphy2 **не использует контекст предложения**, разбор производится индивидуально для каждого слова. Простейший подход к дизамбигуации – взять первый (самый вероятный) разбор из списка. Часто этого достаточно: для большинства словомоформ частотный словарь даёт правильный ответ. Например, `morph.parse('пасти')[0]` вернёт инфинитив глагола «пасти» как наиболее вероятный разбор, хотя всего вариантов шесть. Однако бывают случаи, когда наиболее частотный разбор не подходит в данном тексте. Тогда необходим дополнительный анализ: можно написать алгоритм, учитывающий соседние слова (например, проверять согласование по род/числу/падежу с предыдущим словом для прилагательных и существительных, или использовать вероятности последовательностей – HMM). Pymorphy2 предоставляет инструмент `MorphAnalyzer.parse()` и сортировку по вероятности, но оставляет задачу выбора за пользователем или другой надстройкой.

**Пример использования:**

```python
from pymorphy2 import MorphAnalyzer
morph = MorphAnalyzer()
word = "стали"
parses = morph.parse(word)
for p in parses:
    print(p.word, p.tag, p.normal_form, p.score)
# вывод:
# стали VERB,perf,intr plur,past,indc стать 0.983766
# стали NOUN,inan,femn sing,gent сталь 0.003246
# ... (ещё 3 варианта существительного)
best = parses[0]
print(f"Выбран разбор: {best.tag}, лемма: {best.normal_form}")
```

Здесь мы получим первый разбор как глагол и выберем его как итоговый – таким образом омонимия для слова *«стали»* будет снята в пользу глагола.

**Ограничения:** Pymorphy2 не знает контекст, поэтому может ошибаться там, где менее частотное значение употреблено в тексте. Например, для слова *«плачу»* первым будет разбор *«плакать (я плачу)»* (глагол, 1 л., наст. вр.), а не *«платить (я плачу)»* (глагол, 1 л., наст. вр. от платить), хотя оба варианты весьма вероятны – здесь частотность решит, но контекст может быть критичен. Для надёжного снятия омонимии pymorphy2 часто комбинируют с синтаксическими правилами или статистическими теггерами. Тем не менее, pymorphy2 прост в использовании, быстр (десятки тысяч слов в секунду) и покрывает богатый словарь русского языка.

### 2.2 Natasha (Slovnet Morph)

**Natasha** – это комплексный open-source проект для русского NLP, включающий токенизацию, морфологический и синтаксический анализ, именованные сущности и пр. В контексте морфологии Natasha опирается на модуль **Slovnet Morph** – компактную нейросетевую модель-теггер для русского языка. В отличие от pymorphy2, Slovnet Morph *учитывает контекст*: модель берет последовательность токенов предложения и предсказывает для каждого токена единственный наиболее вероятный морфологический тег. Фактически, Natasha выполняет задачу part-of-speech tagging и морфологической разметки в одной модели, аналогично тому, как это делают SpaCy или UDPipe, но специально для русского. Качество при этом на новостном тексте сравнимо с лучшими решениями (около 98% точности по тегам). Модель Natasha компактна: эмбеддинги \~27 MB и сами веса теггера порядка нескольких мегабайт, что гораздо меньше BERT-моделей, и работает быстро на CPU (например, 25 предложений в секунду в новостях для NER; морфотеггер сопоставим).

**Как работает анализ:** В Natasha после сегментации текста на предложения и токены (модуль **Razdel**) вызывается `NewsMorphTagger` из Slovnet. Он присваивает каждому токену морфологический тег и лемму. Внутренне модель выдает грамматические характеристики в формате Universal Dependencies (UPOS и атрибуты вроде Case, Gender, Number и т.д.). Затем библиотека Natasha дополнительно нормализует леммы через специальный словарь **MorphVocab**, который использует данные pymorphy2. Это позволяет получить литературные начальные формы имен и т.д.

**Снятие омонимии:** Поскольку Natasha выполняет разбор всего предложения сразу, **она автоматически снимает омонимию на основе контекста**. Модель обучена различать, какая часть речи и форма более уместна, глядя на соседние слова. Например, слово *«стали»* в окружении слов «они ... лучше работать» будет размечено как глагол (и лемма «стать»), а в окружении слов «производство ... выросло» – как существительное («сталь»), даже если форма одинаковая. Пользователю не нужно вручную выбирать из вариантов – Natasha выдаёт **единственный** разбор каждого токена. Это упрощает использование: после `doc.tag_morph(morph_tagger)` каждый токен объекта `doc` получает свойства `.pos`, `.feats` (граммемы) и `.lemma`. Таким образом, Natasha решает проблему омонимии на этапе морфологической разметки.

**Пример использования:**

```python
from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, Doc
segmenter = Segmenter()
emb = NewsEmbedding()           # загружаем модельные эмбеддинги
morph_tagger = NewsMorphTagger(emb)
text = "Ветер гнул ветви деревьев"
doc = Doc(text)
doc.segment(segmenter)          # сегментируем на токены
doc.tag_morph(morph_tagger)     # запускаем морфологический теггер
for token in doc.tokens:
    print(token.text, token.pos, token.feats, token.lemma)
# пример вывода для каждого токена:
# "Ветер" NOUN Gender=Masc,... ветер
# "гнул" VERB Tense=Past,... гнуть
# "ветви" NOUN Number=Plur,... ветвь
# "деревьев" NOUN Case=Gen,... дерево
```

Здесь модель поняла, что *«ветер»* – сущ., *«гнул»* – глагол прошедшего времени, *«ветви»* – сущ. мн.ч., *«деревьев»* – сущ. род. падежа мн.ч. Каждое слово получило единственный разбор соответственно контексту.

**Различия с pymorphy2:** Главное отличие – **метод анализа**. Pymorphy2 использует словарь и правила, Natasha – нейросеть (обученную на размеченных корпусах, таких как SynTagRus). Поэтому Natasha может обрабатывать опечатки или новые слова, если их морфологическая структура встречалась в обучении, тогда как pymorphy2 либо не распознает слово, либо попытается угадать по суффиксу. Natasha выдаёт разборы в стандартном формате (UPOS, совместимый с Universal Dependencies), pymorphy2 – в формате OpenCorpora (более подробном для русского, но специфичном). Natasha сразу выбирает один вариант, pymorphy2 – предлагает все и вероятность.

**Комбинированное использование:** Отметим, что в составе Natasha сама лемматизация выполняется с помощью pymorphy2 (MorphVocab) под капотом. То есть эти инструменты не конкурируют, а могут дополнять друг друга. Если нужно максимально точное решение, можно использовать Natasha для контекстного POS-теггинга (снятия омонимии) и pymorphy2 для более полного набора граммем или альтернативных разборов, если вдруг нужны все варианты для анализа.

**Вывод:** *Для задачи простого морфологического анализа художественного текста с минимальной доработкой, Natasha предоставляет удобный высокий уровень (всё в одном: токенизация, разбор, дизамбигуация). Pymorphy2 же может быть использована, если нужна простота установки, высокая скорость или полный контроль над выбором разбора.* Ниже, при формулировке задания, мы можем выбрать любой из этих инструментов. Например, вариант – сначала получить разборы pymorphy2 и взять первый вариант (что проще в реализации), либо воспользоваться Natasha для большей точности на контексте.

**Сравнение Pymorphy2 и Natasha:**

| **Характеристика**   | **Pymorphy2** (словарь)                                                                                                                | **Natasha** (Slovnet Morph)                                                                                                                                           |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Метод анализа**    | Словарь + правила (OpenCorpora); статическая частотная оценка разбора.                                                                 | Нейросетевой теггер (Bi-LSTM/Embeddings) с обучением на размеченном корпусе.                                                                                          |
| **Контекст**         | Не учитывается (анализ по отдельным словам); омонимия не снимается автоматически.                                                      | Учитывается (анализ предложения целиком); омонимия разрешается моделью в процессе.                                                                                    |
| **Выходные данные**  | Все возможные разборы слова (OpencorporaTag) + лемма, с вероятностными весами.                                                         | Единственный разбор на токен (UPOS + граммемы) + лемма; соответствует контексту.                                                                                      |
| **Лемматизация**     | Нормальная форма выдаётся напрямую (например, `Parse.normal_form`).                                                                    | После морфоразметки выполняется отдельный шаг лемматизации через MorphVocab (данные pymorphy2).                                                                       |
| **Скорость**         | \~100 тыс. слов/с в оптимизированной версии (очень быстро, т.к. словарь в памяти).                                                     | Достаточно высокая для модели (\~ десятки предложений/с на CPU), но медленнее pymorphy2 при больших объемах текста.                                                   |
| **Покрытие лексики** | \~3 млн словоформ; умеет угадывать форму незнакомых слов (с возможными ошибками).                                                      | Ограничено формами, представленными в обучающих данных; незнакомые слова обрабатываются по сходству с известными корнями/окончаниями.                                 |
| **Использование**    | Простое: `MorphAnalyzer.parse(word)` – получаем разборы; необходимо самостоятельно разбить текст на слова и выбрать правильный разбор. | Требует инициализации модели: `Doc` + `Segmenter` + `NewsMorphTagger` -> `doc.tag_morph`; выполняет токенизацию и разбор автоматически, упрощая дальнейшую обработку. |

## 3. Лучшие практики: структура данных, Trie и визуализация

При реализации поставленной задачи (морфоанализ текста с построением префиксного дерева слогов и визуализацией) важно продумать структуру хранения данных и выбор инструментов. Рассмотрим рекомендации по каждому этапу.

### 3.1 Структура хранения морфологических данных

Результаты морфологического анализа текста – это, по сути, набор слов (точнее, токенов) с их свойствами. Структурировать их можно по-разному, в зависимости от требований последующей обработки. В нашем случае, конечная цель – построение слогового префиксного дерева, где на узлах хранится информация о словах.

**Выбор единицы хранения:** Если текст обрабатывается построчно или целиком, на первом шаге проводится **токенизация** – разбиение на слова и знаки препинания. В художественном тексте знаки препинания можно либо игнорировать для морфологического анализа, либо обрабатывать отдельно (в контексте задачи морфоанализа их можно опустить, сконцентрировавшись на словах). Лучше всего использовать готовый токенизатор (например, `razdel` из Natasha или регулярное выражение, разбивающее по непробельным символам, учитывая пунктуацию).

**Структура данных:** Рекомендуется создать структуру, связывающую каждое **слово** (в конкретной форме, как в тексте) с его **морфологической информацией**. Возможные варианты:

* **Список объектов**: определить класс `TokenInfo` с полями `word` (токен), `lemma` (лемма), `POS` (часть речи), `feats` (граммемы), `analysis` (полный разбор из библиотеки). Пройти по всем токенам текста, создать объект для каждого. Если важен порядок для вывода, то можно сохранить индекс предложения и позиции слова.
* **Словарь (dict)**: ключ – словоформа, значение – структура с информацией. Но у разных словоформ может быть разная информация, и словарь не сохранит исходный порядок. Он годится, если нужно быстро индексировать по слову (например, для повторяющихся слов).
* **Вложенная структура**: например, список предложений, внутри каждого – список токенов с информацией. Это удобно, если нужно сохранять разграничение предложений (например, для визуализации контекстов), но для нашей задачи (строить глобальный trie по всему тексту) разделение предложений не важно.

С точки зрения последующего построения Trie, нам потребуется множество пар `слово -> морфо-информация`. Поэтому оптимально сформировать, например, список или итератор таких пар. При вставке в префиксное дерево мы будем использовать **слово (его слоги) как ключ**, а в узле хранить ссылку на морфо-информацию (например, тот же объект или, по крайней мере, лемму и часть речи).

**Нормализация данных:** Стоит определиться, должны ли слова храниться в исходной форме или нормализованной. Для построения слогового дерева логичнее использовать **точно такие слоги, как в слове из текста**, чтобы отразить фонетику оригинального текста. Поэтому слова лучше не приводить к начальной форме перед построением дерева – иначе мы потеряем, к примеру, различие «роза» (именит.падеж) и «розы» (род.падеж) в слогах. Пусть каждое слово остаётся в своей форме, со своей слоговой структурой. Морфологическая информация при этом может включать и лемму, и исходную форму – поэтому хранить оба значения.

**Повторяющиеся слова:** В тексте могут встречаться одни и те же слова несколько раз. При построении дерева есть два подхода:

* Вставлять слово каждый раз (дублировать путь). Это неэффективно – лучше этого избежать.
* Вставлять уникальные слова. При первом появлении слова в тексте – добавить его в trie. При следующих – можно либо игнорировать (если достаточно единожды хранить), либо обновлять информацию (например, счётчик частоты в узле).

Лучшей практикой будет сначала собрать **уникальный список слов** (например, через `set`), выполнить для них морфоанализ и построить дерево. Однако если порядок слов важен или если нужно пометить, сколько раз слово было, можно в узле дерева хранить счётчик частоты. Но задача этого не требует явно, поэтому, вероятно, достаточно уникального добавления.

**Хранение морфологических признаков:** Так как на выходе (в визуализации или отчёте) мы можем захотеть видеть, например, часть речи каждого слова, стоит сохранить этот атрибут. Оптимально хранить:

* **Lemma** (лемма, нормальная форма) – для понимания базового слова.
* **POS** (часть речи) – например, существительное (NOUN), глагол (VERB) и т.п.
* **Граммемы** – род, число, падеж, время, лицо и т.д., при необходимости. Можно сохранить строкой (например, тег OpenCorpora или набор признаков UD).
* Возможно, **начальную форму как в тексте** (то есть сам токен, если мы хотим отображать именно его). Но это и так будет ключ в дереве, можно не дублировать.

В pymorphy2 мы получим объект Parse, у которого есть `.tag` (включает часть речи и граммемы) и `.normal_form`. В Natasha – у токена есть `.pos` и `.feats`, `.lemma`. Их можно сразу сохранить в свою структуру.

Итого, **лучший подход**: создать, например, список `tokens_info`, где каждый элемент – dict или объект с полями: `word` (оригинал), `lemma`, `pos`, `features`. Затем этот список (или его уникальные по слову элементы) использовать для построения trie.

### 3.2 Построение префиксного дерева (Trie) по слогам

**Структура trie:** Префиксное дерево удобно представить либо классом, либо вложенными словарями. Класс `TrieNode` может содержать словарь `children` (ключи – слоги, значения – узлы) и, например, поле `data` для хранения данных, когда узел соответствует окончанию слова.

Поскольку слоги – это строки разной длины, которые могут состоять из нескольких букв, они будут единицами перехода. Например, если слово разбито на слоги `["про", "gram", "ма"]`, то в `children` корня будет ключ `"про"` указывающий на узел первого слога. Далее у него в `children` будет ключ `"gram"` и т.д. Необходимо обеспечить, чтобы при вставке слова мы правильно разделяли его на слоги (можно заранее получить список слогов функцией или использовать библиотеку, как описано выше).

**Алгоритм вставки слова в trie:**

1. Начинаем с корневого узла.
2. Берём первый слог слова. Если в `root.children` есть такой ключ, переходим в соответствующий дочерний узел. Если нет – создаём новый узел и добавляем в `children` под этим ключом.
3. Переходим к следующему слогу и повторяем: смотрим в `children` текущего узла. Так продвигаемся по всем слогам.
4. Когда слоги закончились (мы поместили всё слово), помечаем текущий узел как терминальный для слова. Обычно это делают либо флагом `is_end = True`, либо сохранением данных. Здесь важно на терминальном узле сохранить **морфологическую информацию**, связанную с словом. Например, можно сделать `node.data = {lemma: ..., pos: ..., features: ...}`. Если один и тот же слог является окончанием нескольких слов (например, если слово целиком состоит из одного слога, или слова совпадают как слоги полностью), то терминальным узлом будут разные ветви или, если это омоним-совпадение, можно сохранить список данных. Но для уникальных слов этого не случится, кроме как если текст содержал два одинаковых слова – тогда либо данные дублируются, либо можно хранить счётчик.

**Использование стандартных структур:** В Python нет встроенного trie, но можно использовать, например, модуль `collections` с `defaultdict(dict)` для удобства. Есть и сторонние библиотеки (like **pygtrie**), однако писать свою реализацию для учебной задачи приемлемо. Главное – аккуратность и корректность.

**Пример реализации вставки (псевдокод):**

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.data = None   # или self.is_end = False и отдельные поля для данных

root = TrieNode()

def insert_word(word, morph_info):
    syllables = split_into_syllables(word)  # получить список слогов
    node = root
    for syl in syllables:
        if syl not in node.children:
            node.children[syl] = TrieNode()
        node = node.children[syl]
    # по выходе из цикла мы в узле последнего слога
    node.data = morph_info  # сохраняем инфу о слове на терминальном узле
```

Здесь `morph_info` может быть словарём с леммой, частью речи и т.п.

**Рекурсия vs итерация:** Вставку можно делать как итеративно (как выше), так и рекурсивно. Итеративно обычно проще понять и избежать лишнего расхода памяти на глубокий стек вызовов, особенно если слова длинные. Размер слова (в слогах) редко превышает 5-6 для большинства русских слов, но могут быть длинные (по буквам) – слогов 10 и более. Итеративный подход достаточно удобен.

**Построение всего дерева:** Берём подготовленный список уникальных слов с их морфоинформацией. Для каждого вызываем `insert_word(word, info)`. Это займет O(N \* L) операций, где N – число слов, L – среднее число слогов (очень мало по сравнению с буквами, обычно 2-4). Так что эффективность высокая. Потребление памяти – дерево, хранящее каждый слог как отдельный узел. Некоторая экономия памяти может быть достигнута сжатием последовательностей узлов без ветвлений (реализация сжатого префиксного дерева), но в данной задаче это избыточно. Лучше сделать проще и понятно.

**Валидация построения:** Полезно предусмотреть проверку, что все слова вставились правильно. Например, можно затем попытаться пройтись по дереву по слогам каждого слова и убедиться, что на конечном узле хранится правильная лемма. Такие проверки могут быть оформлены как небольшие тесты или отладочный вывод.

### 3.3 Визуализация дерева в виде графа

Для визуализации полученного префиксного дерева (Trie) наглядно и эстетично лучше воспользоваться специализированными инструментами для графов:

* **Graphviz** – классический инструмент для визуализации графов. С помощью Python-библиотеки `graphviz` или генерации `.dot` файла можно описать вершины и ребра дерева, а затем получить картинку (например, в формате PNG or SVG). Graphviz хорошо раскладывает иерархические структуры – у него есть алгоритм **DOT** специально для деревьев (ориентированных ацикличных графов), позволяющий разместить корень наверху, а дочерние узлы ниже.
* **NetworkX + Matplotlib** – позволяет построить граф программно и нарисовать его. Однако, нужно учесть, что стандартный `nx.draw` не всегда удачно размещает узлы для дерева. Можно воспользоваться `networkx.drawing.nx_agraph.graphviz_layout` (если установлен pygraphviz) или `nx.spring_layout` (физический моделируемый расклад). Для небольшого дерева spring\_layout может визуализировать связь, но для четкого дерева лучше Graphviz.

**Что отображать на графе:** Есть два варианта:

* **Отображать каждый узел как точку, а на ребре писать метку слога.** Тогда узлы не подпишутся слогами (можно их индексировать числами или вообще никак), а слоги будут видно на линиях. Это ближе к определению trie (символы на ребрах). Плюс – граф чище (меньше текста внутри узлов).
* **Отображать каждый узел и подписывать его слогом.** Тогда ребра можно оставить без подписи (или указать каким-то стилем, например стрелками). В этом случае путь читается по узлам: корень (пустой) -> узел "про" -> узел "грам" -> узел "ма". Минус – узлы придется именовать уникально, например, комбинировать уровень и слог, чтобы Graphviz не счёл разные узлы с одинаковыми слогами за один (лучше использовать индексы узлов в коде, а лейбл делать слогом).

Оба подхода приемлемы. Первый часто используется для буквенных tries, второй тоже нагляден. Можно выбрать второй для простоты: узел содержит слог (кроме корня – корень можно оставить пустым или пометить как "START").

**Детали визуализации:**

* **Цвета и формы:** Для читаемости можно выделять терминальные узлы (окончание слов) особой формой, например, двойной обводкой или другим цветом. Graphviz позволяет задавать атрибуты узлов (например, `shape=doublecircle` для конечных состояний в автоматах). У нас терминальный узел – узел где `node.data` не None. Их можно выделить, а внутри узла, например, в скобках мелким текстом указать часть речи. Однако, перегружать граф текстом не стоит, иначе он станет тяжёлым для чтения.
* **Надписи:** Если решаем подписывать ребра, в Graphviz делается через `label` на ребре. В networkx – через `nx.draw_networkx_edge_labels`.
* **Размер графа:** Если текст длинный, дерево может получиться большим. Для наглядности можно ограничить визуализацию частью дерева (например, первые N слов или только до определённой глубины). Но в критериях сказано "красивый граф" – значит, ожидается полный, но опрятный. Graphviz при грамотной разметке сам расставит узлы. Можно добавить опцию `rankdir=LR` (если хотим корень слева, дерево растёт вправо) или оставить стандартно (корень сверху). Для слов лучше, возможно, вертикальный вид (корень сверху), чтобы длинные цепочки слогов шли сверху вниз.

**Пример с Graphviz (Dot):**

```
digraph Trie {
    node [shape=circle];
    "" [label="*"];                    // корневой узел, помечаем звёздочкой
    "" -> "про" [label="про"];
    "про" -> "ма" [label="ма"];
    "ма" [shape=doublecircle, label="ма"]; // терминальный узел слова "про-ма"
}
```

Это условный пример синтаксиса: мы создаём узлы с именами (идентификаторами) и метками. В реальности нужно уникальные имена узлов – например, можно генерировать их как пути или использовать адрес объекта.

**NetworkX с matplotlib:** код будет более программным. Надо добавить узел: `G.add_node(node_id, label="слог")` и ребро: `G.add_edge(parent_id, child_id)`. Затем вызвать `pos = graphviz_layout(G, prog="dot")` для координат или `pos = nx.spring_layout(G)` и нарисовать: `nx.draw(G, pos, labels=nx.get_node_attributes(G, 'label'), ...)`. Для edge labels аналогично. Это требует установки pygraphviz or pydot for `graphviz_layout`. Если их нет, spring\_layout может расположить неидеально, но приемлемо для небольших графов.

**Легенда и пояснения:** Хорошей практикой является снабдить визуализацию легендой: пояснить, что означает форма или цвет узла. Например, в сопроводительном тексте (не на самом графе, а в отчете) указать: *«Двойным кругом отмечены узлы, соответствующие концу слова (хранится морфологическая информация). Подписи на ребрах – слоги.»* Такой комментарий поможет проверяющему понять граф.

**Проверка читаемости:** Необходимо убедиться, что на изображении все слоги читаемы (размер шрифта), ничего не налезает друг на друга. Graphviz обычно сам масштабирует. Можно увеличить холст, если граф широкий, или сделать горизонтальную ориентацию. Для вывода желательно использовать формат SVG (вектор) или PNG с высоким разрешением.

## 4. Техническое задание на реализацию

**Задача:** Разработать программу на Python, которая выполняет морфологический разбор входного художественного текста на русском языке, снимает простейшим образом омонимию, сохраняет результаты в префиксном дереве, ключами которого являются слоги слов, и визуализирует это дерево в виде графа.

### 4.1 Функциональные требования

1. **Ввод и предобработка:** Программа должна принимать на вход текстовый файл с художественным текстом (например, рассказ "Воры" А.П. Чехова во вложении). Необходимо корректно прочитать файл (учесть кодировку UTF-8) и разбить текст на отдельные токены слов.

   * Следует удалить из рассмотрения или отдельно обработать знаки пунктуации (точки, запятые, тире и пр.), числовые символы и прочие не-алфавитные токены – они не нуждаются в морфологическом анализе для данной задачи.
   * Предусмотреть, что текст может содержать заглавные буквы в начале предложений или именах: либо привести слова к нижнему регистру перед анализом, либо настроить анализатор на регистронезависимый режим (pymorphy2 учитывает и прописные буквы, Natasha тоже, но для унификации лучше привести к нижнему регистру).
   * На выходе этапа должно получиться последовательность (или список) слов для анализа. Желательно сохранить также их порядок для потенциального сопоставления с исходным текстом (например, через индексы), хотя это не критично для задачи построения дерева.

2. **Морфологический анализ:** Для каждого полученного слова должен быть выполнен морфологический разбор с помощью одной из библиотек: `pymorphy2` или `Natasha` (Slovnet Morph).

   * **Если используется pymorphy2:** вызвать `MorphAnalyzer.parse(word)` и получить список разборов.

     * Из списка выбрать один разбор. Достаточно выбрать первый (самый вероятный) вариант – это и будет простейшее снятие омонимии.
     * Из выбранного разбора извлечь как минимум: лемму (`normal_form`) и часть речи (`tag.POS` или строковое представление тега). При необходимости извлечь другие граммемы (род, число, падеж и пр. из `tag`).
   * **Если используется Natasha:** токенизировать предложение (Natasha `Segmenter`), проанализировать `NewsMorphTagger`. Natasha сама выдаст для каждого токена однозначный разбор, так что дополнительный выбор не требуется. Для каждого токена получить `token.lemma`, `token.pos` и `token.feats` (граммемы).
   * В результате должен быть сформирован набор структур (например, словарей) вида: `{"word": "...", "lemma": "...", "pos": "...", "features": "..."}` для каждого слова. Этот набор можно хранить в списке, соответствующем порядку слов, или в словаре по самим словам (если слова уникальны). Рекомендуется также сохранить только **уникальные** слова для дальнейшего построения дерева, чтобы не дублировать узлы.

3. **Снятие омонимии:** Принцип дизамбигуации – **простой**:

   * В случае pymorphy2 – берется наиболее вероятный разбор (т.е. фактически выбирается одна интерпретация слова).
   * В случае Natasha – модель уже выбрала интерпретацию автоматически.
   * Никакого сложного анализа (участия синтаксического парсера или семантики) не требуется. Тем не менее, выбранный подход должен обеспечить логичность результата: например, если слово *«плачу»*, pymorphy2 отдаст первый вариант *«плакать»*, и программа возьмет его, даже если в контексте имелось в виду *«платить»*. Это приемлемо для **простейшего** снятия омонимии, но в отчёте следует упомянуть о подобных случаях (можно не исправлять, но понимать ограничение).

4. **Разбиение на слоги:** Необходимо реализовать функцию, которая получает строку (слово) и возвращает список слогов.

   * Можно использовать собственную реализацию на основе правил (как описано в разделе 1.6). Достаточно корректно обрабатывать стандартные случаи (один согласный между гласными -> относится к следующему слогу, два и более -> разбиение между ними, и учесть `й`, `ь`, `ъ` как неполноценные гласные).
   * Либо применить библиотеку `rusyllab` для надёжности, чтобы получить слоги без детальной ручной прописки правил. В случае использования библиотеки, убедиться, что она подключается (она не в PyPI, но устанавливается из GitHub). Допустимо также подключить простой список правил или даже зашитый в код список гласных и последовательностей.
   * Функция должна корректно работать для любого русского слова, встречающегося в тексте. Особое внимание: односложные слова (возвращается целое слово как один слог), сочетания с ё (ё – гласная), й (например, «йод» – вероятно один слог, «майор» – ма-йор), ь/ъ (разделительные знаки, не образуют слог сами по себе).

5. **Построение Trie:** На основе списка уникальных слов текста (и их морфологических данных) построить префиксное дерево, в котором ключи – последовательности слогов:

   * Создать класс или использовать вложенные словари для представления узлов дерева. Каждый узел хранит детей (слог -> узел).
   * Корневой узел соответствует пустому префиксу (не содержит слога).
   * Каждый путь от корня до листа соответствует слову. **Терминальные узлы** – те, на которых слово заканчивается – должны содержать сохранённую морфологическую информацию о слове.
   * Необходимо вставить в дерево все слова. При вставке обрабатывать уже существующие ветви (не дублировать узлы слогов, если путь уже частично пройден).
   * В итоге должна получиться единая структура, хранящая все слова текста. Можно дополнительно хранить в терминальном узле саму лемму или оригинальное слово для удобства (например, если потом понадобятся подписи полные).

6. **Визуализация дерева:** Построенное префиксное дерево требуется визуализировать в понятном графическом виде:

   * Использовать либо Graphviz, либо NetworkX (с matplotlib) для отрисовки графа. Граф должен отражать структуру trie: узлы и соединяющие их рёбра.
   * **Отображение слогов:** каждый переход между узлами помечается слогом. Рекомендуется именно показать слоги либо на ребрах, либо в узлах. Например, можно выбрать: узлы – кружочки без текста, а над каждым ребром написать слог. Либо узлы – с подписью слога. Главное, чтобы при чтении от корня к листу можно было составить слово по слогам.
   * **Отображение корня:** корень может быть показан как пустой (например, либо без метки, либо специальным символом типа "\*" или "ROOT"). От него исходят рёбра первых слогов каждого слова.
   * **Концевые узлы:** выделить узлы, соответствующие концам слов. Например, другим цветом или двойной рамкой. Можно внутри них указать дополнительную информацию. **Обязательное требование** – визуально отличить конец слова. Если ребра подписаны слогами, можно, например, концевой узел закрасить.
   * **Дополнительная информация:** По возможности, отобразить на графе часть речи или лемму в конечных узлах. Однако, нужно следить за читаемостью: если слово длинное, его лемма не сильно отличается, и так ясно; часть речи можно обозначить аббревиатурой (N, V, ADJ, etc.). Можно также сделать выноски или таблицу отдельно, но это усложнит. Допустимо ограничиться тем, что в узле-последнем слоге написать, например, лемму курсивом под слогом.
   * **Эстетика:** граф должен быть **разборчивым и не перегруженным**. Если дерево очень большое (более 50-100 узлов), возможно, лучше вывести его в большом формате (например, масштабировать или разделить на подграфы). Для рассказа "Воры" объем слов умеренный, дерево будет содержать несколько сотен узлов – Graphviz справится. Можно применить настройку рангов для уровней слогов, чтобы все узлы одного слогового уровня были на одной горизонтали – это сделает граф упорядоченным.
   * Результат визуализации сохранить в файл (PNG/SVG) и/или отобразить в рамках работы программы. Граф должен выглядеть аккуратно: надписи не налезают друг на друга, шрифт достаточно крупный для чтения слогов. Желательно подобрать цветовую схему (например, синие узлы, черные ребра, терминальные узлы зеленым).

7. **Вывод результатов:** Помимо графического файла, программа может в консоль или отдельный текстовый файл вывести сводку:

   * Общее количество слов обработано, сколько из них уникальных.
   * Для некоторых слов – их разбор (пример, 5-10 слов с их леммами и частями речи, просто чтобы видеть текстуальный результат морфоанализа).
   * Возможно, информацию о глубине получившегося дерева (максимальное число слогов в самом длинном слове, среднее число слогов на слово).
   * Эти текстовые данные помогут проверить правильность логики, но основным финальным представлением дерева будет граф.

### 4.2 Нефункциональные требования (качество кода)

Помимо корректной функциональности, код решения должен удовлетворять критериям качества и стиля:

* **Корректность работы:** Программа должна правильно обрабатывать входные данные. Например, не «падать» на нетипичных символах (ём, ё, длинное тире или кавычки елочки в тексте и т.п.). Желательно предусмотреть фильтрацию/замену редких символов. Если встречается слово с дефисом (например, "когда-нибудь"), его можно либо оставить целиком, либо разделить на части (по смыслу лучше разобрать отдельно "когда" и "нибудь").

* **Эффективность алгоритма:** Для заданного объема текста (рассказ, несколько страниц) решение должно работать быстро. Использование словарей, множества для уникальности, и встроенных функций Python предпочтительнее, чем вложенные циклы по строкам. Алгоритмическая сложность основных операций:

  * Морфоанализ библиотечный (оптимизирован в самих библиотеках).
  * Построение trie – линейное от суммы длин слов, что эффективно. Недопустимо, например, для каждого узла делать дорогостоящие операции вроде конкатенации длинных строк в цикле (это могло бы замедлить).
  * Визуализация – может быть узким местом, но для разумного размера графа это не критично. Однако, не следует пытаться вручную рассчитывать координаты узлов – лучше доверить это библиотеке.

* **Стиль кода:** Код должен быть написан в понятном стиле, с разбивкой на функции по логике:

  * Функция для чтения и токенизации текста;
  * Функция для морфологического анализа списка слов;
  * Функция для деления одного слова на слоги;
  * Класс/функции для управления Trie (вставка слов, возможно поиск слов для теста);
  * Функция для генерации визуализации.
  * И основной блок, который вызывает эти компоненты в нужном порядке.

  Имена переменных и функций должны быть осмысленными (например, `text = read_text(file_path)`, `tokens = tokenize(text)`, `morph_infos = analyze_tokens(tokens)`, `trie = build_trie(morph_infos)`, `draw_trie(trie)` – говорящие названия). Соблюдать PEP8: отступы 4 пробела, длина строки \~< 120 символов, и т.д. Избегать чрезмерно длинных функций – лучше несколько поменьше, чем одна на 200 строк.

* **Использование библиотек:** Поощряется максимальное применение готовых библиотек там, где это рационально:

  * Морфологический анализ – доверить pymorphy2 или Natasha, а не пытаться вручную кодировать словари частей речи.
  * Деление на слоги – если доступен rusyllab или аналог, можно использовать, чтобы не тратить слишком много времени на отладку правил (но при этом понимать правила).
  * Визуализация – использовать Graphviz или networkx; **не рисовать граф вручную через ASCII** или кустарные методы.

  В то же время, не следует тянуть тяжёлые фреймворки без необходимости. Например, подключать полный SpaCy ради морфологии было бы избыточно, когда есть pymorphy2. То есть **"библиотечность"** решения должна проявляться в разумном использовании существующих инструментов, без "изобретения велосипеда", но и без перегрузки проекта ненужным.

* **Отсутствие "алгоритмической криворукости":** Это полу-шутливое требование означает, что решение не должно быть сделано небрежно или грубо с точки зрения алгоритмов. Конкретные анти-примеры:

  * Грубое нарушение DRY: дублирование одного и того же кода в нескольких местах (лучше вынести в функцию).
  * Сложность, ухудшающая понимание: например, использование глобальных переменных для передачи данных между функциями, когда можно передать параметром.
  * Очевидно неэффективные конструкции: тройной вложенный цикл по всем словам и всем узлам дерева на каждом шаге вставки (вместо прямого спуска по дочерним узлам).
  * Пренебрежение удобством: отсутствие комментариев, где это необходимо (например, около сложного кусочка кода с обработкой `ъ` и `ь` в слогах стоит пояснить, что делается).

  Код должен быть не только рабочим, но и в известной мере оптимальным и поддерживаемым.

### 4.3 Критерии приёмки

Приемочное тестирование решения будет проводиться по следующим критериям:

1. **Корректность обработки входных данных:** Программа успешно читает файл с текстом "Воры" (и аналогичные тексты), разделяет текст на слова, игнорируя/удаляя лишние символы. На выходе этапа – полный список слов, соответствующий тексту, без потерь и без лишних элементов. Проверяется, что, например, количество слов соответствует ожиданию, слова не содержат прилегающих знаков препинания (они отделены).

2. **Правильность морфологического анализа:** Для каждого слова из текста получена правильная (или по крайней мере обоснованная) морфологическая информация. Лемматизация должна быть верной (за редкими исключениями, если слово не распознано – тогда допустимо, что лемма равна исходному слову). Части речи должны соответствовать контексту: например, имя собственное "Любка" может анализатором быть помечено как имя существительное, имя собственное, жен. род – это верно. Если слово неизвестно (например, "Мерик" – имя персонажа), pymorphy2 может пометить его как просто имя сущ. муж. род (что нормально). Тестировщик выборочно проверит 5-10 слов и убедится, что тег совпадает с реальным значением в тексте.

3. **Снятие омонимии:** В случаях омонимичных слов проверяется, что программа не оставляет несколько вариантов, а выбрала один. Например, слово "стал" в тексте должно быть либо глаголом, либо существительным – в структуре данных должен присутствовать только один разбор. Если контекст явно указывает, предпочтённый разбор должен совпадать. (В рамках простого подхода допускается, что выбрана неправильная интерпретация для какого-то слова, но это не должно приводить к сбою работы – это оценивается лояльно, главное отсутствие множества вариантов в выходных данных).

4. **Корректность построения префиксного дерева:** Структура Trie будет проверена двумя способами:

   * **Полнота:** все уникальные слова текста должны присутствовать в дереве. Не должно быть пропущенных слов. Количество терминальных узлов должно совпадать с числом уникальных слов.
   * **Правильность узлов:** каждая ветвь от корня до терминала должна образовывать последовательность слогов, которая складывается именно в то слово, которое на терминале хранится. Будут выбраны несколько слов (начало, середина, конец списка) и проверено прохождение по дереву: например, слово "балалайка" -> слоги "ба-ла-лай-ка" – убедиться, что от корня есть дети "ба" -> потом "ла" -> "лай" -> "ка" и в последнем узле data имеет lemma "балалайка" (или "балалайка" как лемма, pos = NOUN).
   * **Отсутствие лишних узлов:** Если одно и то же слогосочетание не встречается ни в одном слове, его не должно быть. (Это автоматически выполняется, если алгоритм вставки корректен). Если, допустим, есть узел с data=None и без детей – такого не должно остаться (пустой узел ни к чему).
   * **Корневой узел** должен иметь детей, равных числу различных первых слогов встреченных слов. Например, если текст начинался на "Любка", "Мерик", "фельдшер" – у корня должны быть дети "лю", "ме", "фельд" и т.д.

5. **Визуализация дерева:** Графическая визуализация будет рассмотрена на предмет:

   * **Читаемости:** все надписи (слоги, возможно леммы/теги) различимы, не обрезаны. Узлы/рёбра не сливаются визуально.
   * **Правильного содержания:** граф соответствует дереву. Будут выбраны несколько путей на графе и сравнение с реальными словами. Слоги на пути должны давать слово. Если узел помечен как конец слова, это слово действительно есть в тексте.
   * **Оформления:** Наличие легенды или условных обозначений (например, в отчёте указано, что зелёные узлы – окончания слов, на ребрах слоги). Граф не обязан быть художественно изысканным, но структурно опрятным. Не должно быть, к примеру, пересекающихся линий в большом беспорядке (для дерева это маловероятно при использовании DOT-алгоритма).
   * **Формат вывода:** Должен быть представлен либо в отчёте, либо отдельным файлом. Если файл – убедиться, что он открывается. В случае автоматической проверки, можно предусмотреть сохранение изображения программно.

6. **Качество кода и архитектуры:** Код будет просмотрен рецензентом на предмет:

   * Выполнения требований стиля (разумные имена, разбитие на функции/модули).
   * Наличие комментариев, где сложно. Например, перед реализацией функции слогоделения должен быть комментарий о принципе.
   * Отсутствие грубых неэффективных решений: например, если будет обнаружено построение trie через конкатенацию строк в цикле (что создаёт много лишних объектов) или дублирование морфоанализа одного слова многократно, это снизит оценку.
   * Использование библиотек: должна быть задействована хотя бы одна из упомянутых библиотек для морфологии. Нельзя использовать, скажем, только собственноручно составленный словарь – это не соответствует заданию.
   * Код должен быть структурирован так, что его легко запускать и читать. Например, иметь условный блок `if __name__ == "__main__":` для запуска основного сценария (не разбросать исполняемый код между функциями).

7. **Документация и результаты:** В финальном отчёте (или ноутбуке) должны быть отражены:

   * Краткое пояснение, какие библиотеки использованы и почему (например: *"Используем pymorphy2 для морфоанализа, так как он прост в установке и даёт достаточную точность для наших целей."*).
   * Примеры работы: например, приведен кусочек таблицы: слово – лемма – часть речи (для нескольких токенов текста). Это показывает, что программа реально сделала разбор.
   * Визуализация дерева (вставлена картинка или ссылка на неё).
   * Комментарии по тому, как проверялись слоги (возможно, перечисление нескольких слов с их делением на слоги для демонстрации корректности алгоритма).
   * Указание, что все пункты задания выполнены: морфоанализ -> есть, омонимия снята -> да, trie построено -> да, граф выведен -> да, и код отлажен.

Каждый из перечисленных пунктов будет проверяться. Успешное выполнение всех требований означает, что проект удовлетворяет техническому заданию полностью. Особое внимание будет обращено на соответствие реализации поставленной задаче (не упущены ли какие-либо ключевые этапы) и на презентацию результатов (чтобы сторонний читатель мог понять, что сделано и увидеть доказательства этого).
